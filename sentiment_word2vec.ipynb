{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "This notebook is used to learn the embeddings of words in a dataset using the word2vec model."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from datasets import load_dataset\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "import torch.optim as optim\n",
            "import torchtext\n",
            "import nltk\n",
            "from nltk.corpus import stopwords\n",
            "from datasets import load_from_disk\n",
            "import numpy as np\n",
            "import tqdm\n",
            "import pandas as pd\n",
            "from datasets import Dataset\n",
            "import collections"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "seed = 257\n",
            "\n",
            "np.random.seed(seed)\n",
            "torch.manual_seed(seed)\n",
            "torch.cuda.manual_seed(seed)\n",
            "torch.backends.cudnn.deterministic = True"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Prepare the data\n",
            "\n",
            "We begin by tokenizing and cleaning the data. This process consists of removing punctuation, numbers, and stop words."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# load the dataset\n",
            "train_data, test_data = load_dataset(\"yelp_polarity\", split=[\"train\", \"test\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# tokenize the dataset\n",
            "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
            "\n",
            "\n",
            "def tokenize(obs, tokenizer, max_length=512):\n",
            "    \"\"\"\n",
            "    Tokenize an observation\n",
            "    max_length: the maximum length of the tokenized sequence\n",
            "    \"\"\"\n",
            "    return {\"tokens\": tokenizer(obs[\"text\"])[:max_length]}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# remove stopwords and punctuation\n",
            "stop_words = stopwords.words(\"english\")\n",
            "\n",
            "\n",
            "def remove_stopwords(obs):\n",
            "    \"\"\"\n",
            "    Removes stopwords from tokens for each obs in Dataset\n",
            "    \"\"\"\n",
            "    obs[\"tokens\"] = [word for word in obs[\"tokens\"] if word not in stop_words]\n",
            "    return obs\n",
            "\n",
            "\n",
            "def remove_punctuation(obs):\n",
            "    \"\"\"\n",
            "    Removes punctuation from tokens for each obs in Dataset\n",
            "    \"\"\"\n",
            "    obs[\"tokens\"] = [word for word in obs[\"tokens\"] if word.isalpha()]\n",
            "    return obs\n",
            "\n",
            "\n",
            "def tokenize_and_clean(obs):\n",
            "    \"\"\"\n",
            "    Tokenize, remove stopwords and punctuation from observation\n",
            "    \"\"\"\n",
            "    tokens = tokenizer(obs[\"text\"][:512])\n",
            "    tokens = [word for word in tokens if word not in stop_words]\n",
            "    tokens = [word for word in tokens if word.isalpha()]\n",
            "    return {\"tokens\": tokens}\n",
            "\n",
            "\n",
            "# train_data = train_data.map(remove_stopwords)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# tokenizer(train_data[0][\"text\"][:512])\n",
            "train_data = train_data.map(tokenize_and_clean)\n",
            "test_data = test_data.map(tokenize_and_clean)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# train_data.save_to_disk(\"/datasets/yelp_polarity_train\")\n",
            "# train_data = load_from_disk(\"/datasets/yelp_polarity_train/\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now that our data has been tokenized and cleaned, we can create a validation set."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# validation data\n",
            "train_valid_data = train_data.train_test_split(test_size=0.25)\n",
            "train_data = train_valid_data[\"train\"]\n",
            "valid_data = train_valid_data[\"test\"]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "From the training data, we now proceed to create a vocabulary comprised of the training data's unique words (if they appear more than 75 times)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# creating the vocabulary\n",
            "special_tokens = [\"<unk>\"]\n",
            "\n",
            "# setting a minimum frequency for the tokens ... 75 times in 420,000 sentences is not a lot\n",
            "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
            "    train_data[\"tokens\"], specials=special_tokens, min_freq=75\n",
            ")\n",
            "vocab.set_default_index(vocab[\"<unk>\"])\n",
            "len(vocab)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now that we have the vocabulary, we can numerically encode the words in the training data."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def numericalize_example(obs, vocab):\n",
            "    ids = vocab.lookup_indices(obs[\"tokens\"])\n",
            "    return {\"ids\": ids}\n",
            "\n",
            "\n",
            "train_data = train_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
            "valid_data = valid_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
            "test_data = test_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now that we have numericalized the data, we can create word pairs for the skip-gram model. To finalize the numericalization process, we'll transform `x` and `y` into PyTorch tensors."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_word_pairs(sentence, window_size=3):\n",
            "    \"\"\"\n",
            "    Generate word pairs from a sentence\n",
            "    \"\"\"\n",
            "    for i, ids in enumerate(sentence):\n",
            "        for j in range(1, window_size + 1):\n",
            "            if i + j < len(sentence):\n",
            "                yield (sentence[i], sentence[i + j])\n",
            "            if i - j >= 0:\n",
            "                yield (sentence[i], sentence[i - j])\n",
            "\n",
            "\n",
            "def extract_pairs(dataset):\n",
            "    \"\"\"\n",
            "    Extract word pairs from dataset\n",
            "    \"\"\"\n",
            "    for i, obs in enumerate(dataset):\n",
            "        yield from get_word_pairs(obs[\"ids\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# convert the new training data to a dataset from a DataFrame\n",
            "new_train = extract_pairs(train_data)\n",
            "new_train = pd.DataFrame(new_train, columns=[\"x\", \"y\"])\n",
            "new_train = Dataset.from_pandas(new_train)\n",
            "new_train = new_train.with_format(type=\"torch\", columns=[\"x\", \"y\"])\n",
            "\n",
            "# convert the new validation data to a dataset from a DataFrame\n",
            "new_valid = extract_pairs(valid_data)\n",
            "new_valid = pd.DataFrame(new_valid, columns=[\"x\", \"y\"])\n",
            "new_valid = Dataset.from_pandas(new_valid)\n",
            "new_valid = new_valid.with_format(type=\"torch\", columns=[\"x\", \"y\"])\n",
            "\n",
            "# # convert the new test data to a dataset from a DataFrame\n",
            "new_test = extract_pairs(test_data)\n",
            "new_test = pd.DataFrame(new_test, columns=[\"x\", \"y\"])\n",
            "new_test = Dataset.from_pandas(new_test)\n",
            "new_test = new_test.with_format(type=\"torch\", columns=[\"x\", \"y\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# new_train.save_to_disk(\"/datasets/yelp_polarity_train_torchpairs\")\n",
            "# new_train = load_from_disk(\"/datasets/yelp_polarity_train_torchpairs\")\n",
            "\n",
            "# new_valid.save_to_disk(\"/datasets/yelp_polarity_valid_torchpairs\")\n",
            "# new_valid = load_from_disk(\"/datasets/yelp_polarity_valid_torchpairs\")\n",
            "\n",
            "# new_test.save_to_disk(\"/datasets/yelp_polarity_test_torchpairs\")\n",
            "# new_test = load_from_disk(\"/datasets/yelp_polarity_test_torchpairs\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the RNN, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see here."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The final step in preparing the data is to create a DataLoader and batch the data. For each batch, we'll create a single tensor for the input data and another for the output data. The input data will contain the indexes of the center words and the output data will contain the indexes of the context words."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# collate function\n",
            "def get_collate_fn():\n",
            "    def collate_fn(batch):\n",
            "        \"\"\"\n",
            "        Collate function for the DataLoader\n",
            "        \"\"\"\n",
            "        batch_x = []\n",
            "        batch_y = []\n",
            "        for _, obs in enumerate(batch):\n",
            "            batch_x.append(obs[\"x\"])\n",
            "            batch_y.append(obs[\"y\"])\n",
            "\n",
            "        batch = {\"x\": batch_x, \"y\": batch_y}\n",
            "\n",
            "        return batch\n",
            "\n",
            "    return collate_fn"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we shall define a function which returns our actual data loader. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_data_loader(dataset, batch_size=64, shuffle=False):\n",
            "    \"\"\"\n",
            "    Get a DataLoader for the dataset\n",
            "    \"\"\"\n",
            "    collate_fn = get_collate_fn()\n",
            "    data_loader = torch.utils.data.DataLoader(\n",
            "        dataset=dataset,\n",
            "        batch_size=batch_size,\n",
            "        shuffle=shuffle,\n",
            "        collate_fn=collate_fn,\n",
            "    )\n",
            "    return data_loader"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we get the data loaders for each of our train, validation and test sets. After doing so, we can continue onto building our model."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "batch_size = 64\n",
            "\n",
            "train_loader = get_data_loader(new_train, batch_size=batch_size, shuffle=True)\n",
            "valid_loader = get_data_loader(new_valid, batch_size=batch_size, shuffle=False)\n",
            "test_loader = get_data_loader(new_test, batch_size=batch_size, shuffle=False)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Building the Model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Our model will consist of an embedding layer that will represent words in a lower-dimensional space seeking to capture semantic relations between words. We can understand these as the \"sentiment analysis features\" we seek to extract. Finally, the output layer will be a softmax output layer of size `vocab_size`. The output of the model will eventually be discarded, as we are only interested in the hidden layer weights."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "class Word2Vec(nn.Module):\n",
            "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
            "        super(Word2Vec, self).__init__()\n",
            "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
            "        self.fc1 = nn.Linear(embedding_dim, hidden_size)\n",
            "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
            "        pass\n",
            "\n",
            "    def forward(self, x):\n",
            "        embeds = self.embeddings(x)\n",
            "        out = F.relu(self.fc1(embeds))\n",
            "        out = self.fc2(out)\n",
            "        log_probs = F.log_softmax(out, dim=1)\n",
            "        return log_probs"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "vocab_size = len(vocab)\n",
            "embedding_dim = 10\n",
            "hidden_size = 10\n",
            "\n",
            "model = Word2Vec(vocab_size, vocab_size, hidden_size)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we have to define the loss function and the optimizer. Because we have used a log softmax output, we will use the negative log likelihood loss. We will use the Adam optimizer."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# loss function and optimizer\n",
            "criterion = nn.NLLLoss()\n",
            "optimizer = optim.Adam(model.parameters())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "print(device)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = model.to(device)\n",
            "criterion = criterion.to(device)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we have to define the training loop."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# training function\n",
            "def train(data_loader, model, criterion, optimizer, device):\n",
            "    \"\"\"\n",
            "    Train the model\n",
            "    \"\"\"\n",
            "    model.train()\n",
            "    epoch_losses = []\n",
            "    for batch in tqdm.tqdm(data_loader, desc=\"Training...\"):\n",
            "        x = torch.stack(batch[\"x\"]).to(device)\n",
            "        y = torch.stack(batch[\"y\"]).to(device)\n",
            "        prediction = model(x)\n",
            "        loss = criterion(prediction, y)\n",
            "        optimizer.zero_grad()\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "        epoch_losses.append(loss.item())\n",
            "    return np.mean(epoch_losses)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we can define a validate loop which we'll use to measure validation performance."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# validation function\n",
            "def evaluate(data_loader, model, criterion, device):\n",
            "    \"\"\"\n",
            "    Evaluate the model\n",
            "    \"\"\"\n",
            "    model.eval()\n",
            "    epoch_losses = []\n",
            "    with torch.no_grad():\n",
            "        for batch in tqdm.tqdm(data_loader, desc=\"Evaluating...\"):\n",
            "            x = torch.stack(batch[\"x\"]).to(device)\n",
            "            y = torch.stack(batch[\"y\"]).to(device)\n",
            "            prediction = model(x)\n",
            "            loss = criterion(prediction, y)\n",
            "            epoch_losses.append(loss.item())\n",
            "    return np.mean(epoch_losses)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Our model is ready to be trained. We can now train the model and measure its performance on the validation set."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "num_epochs = 10\n",
            "best_valid_loss = float(\"inf\")\n",
            "\n",
            "metrics = collections.defaultdict(list)\n",
            "\n",
            "for epoch in range(num_epochs):\n",
            "    train_loss = train(train_loader, model, criterion, optimizer, device)\n",
            "    valid_loss = evaluate(valid_loader, model, criterion, device)\n",
            "    metrics[\"train_loss\"].append(train_loss)\n",
            "    metrics[\"valid_loss\"].append(valid_loss)\n",
            "\n",
            "    if valid_loss < best_valid_loss:\n",
            "        best_valid_loss = valid_loss\n",
            "        torch.save(model.state_dict(), \"word2vec-model.pt\")\n",
            "\n",
            "    print(\n",
            "        f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\"\n",
            "    )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "embeddings = model.embeddings.weight.data.cpu().numpy()\n",
            "np.save(\"word2vec-embeddings.npy\", embeddings)\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "base",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
