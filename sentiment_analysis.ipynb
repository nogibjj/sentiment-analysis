{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Sentiment Analysis\n",
            "\n",
            "### ECE590 Homework assignment 2\n",
            "Name: Javier Cervantes\n",
            "net id: jc1010\n",
            "\n",
            "We are interested in sentiment analysis. Given a short document, we wish to assess whether the corresponding sentiment is positive (label ùë¶=1 ) or negative (label ùë¶=0). The assignment is as follows: \n",
            "\n",
            "1. For every word, we will learn a corresponding d-dimensional vector $x_i \\in \\mathbb{R}^d$ for word $i$ in the vocabulary. \n",
            "\n",
            "2. Assume that there are $M_j$ words in document $j$. The feature vector for this document is $f_j = \\frac{1}{M_j} \\sum_{i=1}^{M_j} x_{(m, j)}$ such that $x_{(m, j)}$ is the d-dimensional vector for the m-th word in document j.\n",
            "\n",
            "3. The probability of positive sentiment for document j is modeled as $P(y_j = 1 | f_j) = \\sigma[w \\cdot f_j + b]$ where $\\sigma$ is the sigmoid function, $w \\in \\mathbb{R}^d$ is the weight vector and $b \\in \\mathbb{R}$ is the bias."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "from datasets import load_dataset\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "import torch.optim as optim\n",
            "import torchtext\n",
            "import nltk\n",
            "from nltk.corpus import stopwords\n",
            "from datasets import load_from_disk\n",
            "import numpy as np\n",
            "import tqdm\n",
            "import pandas as pd\n",
            "from datasets import Dataset\n",
            "import collections"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "seed = 257\n",
            "\n",
            "np.random.seed(seed)\n",
            "torch.manual_seed(seed)\n",
            "torch.cuda.manual_seed(seed)\n",
            "torch.backends.cudnn.deterministic = True"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Prepare the data\n",
            "\n",
            "We begin by tokenizing and cleaning the text. In this process, we'll remove punctuation, convert to lowercase, and remove stopwords. I believe that it's worth noting that removing stop words might be problematic in some model designs. For this particular model, which doesn't take into account word order, removing words like \"not\" should not affect the model's performance because the model won't have the capability of identifying where upon a given document the word \"not\" is located."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "# load the dataset\n",
            "train_data, test_data = load_dataset(\"yelp_polarity\", split=[\"train\", \"test\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "# tokenize the dataset\n",
            "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
            "\n",
            "\n",
            "def tokenize(obs, tokenizer, max_length):\n",
            "    \"\"\"\n",
            "    Tokenize an observation\n",
            "    max_length: the maximum length of the tokenized sequence\n",
            "    \"\"\"\n",
            "    return {\"tokens\": tokenizer(obs[\"text\"])[:max_length]}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[nltk_data] Downloading package stopwords to\n",
                  "[nltk_data]     C:\\Users\\cerva\\AppData\\Roaming\\nltk_data...\n",
                  "[nltk_data]   Package stopwords is already up-to-date!\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "nltk.download(\"stopwords\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "# remove stopwords and punctuation\n",
            "stop_words = stopwords.words(\"english\")\n",
            "\n",
            "\n",
            "def remove_stopwords(obs):\n",
            "    \"\"\"\n",
            "    Removes stopwords from tokens for each obs in Dataset\n",
            "    \"\"\"\n",
            "    obs[\"tokens\"] = [word for word in obs[\"tokens\"] if word not in stop_words]\n",
            "    return obs\n",
            "\n",
            "\n",
            "def remove_punctuation(obs):\n",
            "    \"\"\"\n",
            "    Removes punctuation from tokens for each obs in Dataset\n",
            "    \"\"\"\n",
            "    obs[\"tokens\"] = [word for word in obs[\"tokens\"] if word.isalpha()]\n",
            "    return obs\n",
            "\n",
            "\n",
            "def tokenize_and_clean(obs, max_length):\n",
            "    \"\"\"\n",
            "    Tokenize, remove stopwords and punctuation from observation\n",
            "    \"\"\"\n",
            "    tokens = tokenizer(obs[\"text\"][:max_length])\n",
            "    tokens = [word for word in tokens if word not in stop_words]\n",
            "    tokens = [word for word in tokens if word.isalpha()]\n",
            "    return {\"tokens\": tokens}\n",
            "\n",
            "\n",
            "# train_data = train_data.map(remove_stopwords)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Working under the assumption that a document's sentiment can be identified rather quickly, I've set the maximum length of a document to 100 words. This is a hyperparameter that can be adjusted to improve the model's performance."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "max_length = 100\n",
            "\n",
            "# tokenizer(train_data[0][\"text\"][:512])\n",
            "\n",
            "train_data = train_data.map(tokenize_and_clean, fn_kwargs={\"max_length\": max_length})\n",
            "\n",
            "# test_data = test_data.map(tokenize_and_clean)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "# train_data.save_to_disk(\"/datasets/yelp_polarity_train\")\n",
            "# train_data = load_from_disk(\"/datasets/yelp_polarity_train/\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now that our data has been tokenized and cleaned, we can create a validation set."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "# validation data\n",
            "train_valid_data = train_data.train_test_split(test_size=0.25)\n",
            "train_data = train_valid_data[\"train\"]\n",
            "valid_data = train_valid_data[\"test\"]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "From the training data, we now proceed to create a vocabulary comprised of the training data's unique words. Given the large number of documents in the training set, I'll add a minimum frequency threshold of 30 for every word to be included in the vocabulary. Given the large number of observations we have available, removing words that appear only a few times should not affect the model's performance.\n",
            "\n",
            "Also very important in the creation of our vocabulary is to add a couple of special tokens: one for padding and one for unknown words. The padding token will be used to make all documents the same length, and the unknown token will be used to catch words that are not in the vocabulary."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "7827"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# creating the vocabulary\n",
            "special_tokens = [\"<unk>\", \"<pad>\"]\n",
            "\n",
            "# setting a minimum frequency for the tokens ... 30 times in 420,000 sentences is not a lot\n",
            "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
            "    train_data[\"tokens\"], specials=special_tokens, min_freq=30\n",
            ")\n",
            "vocab.set_default_index(vocab[\"<unk>\"])\n",
            "len(vocab)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "We now have a vocabulary of 7,827 unique words (including the special tokens). The Vocab object has a method that is used to identify unknown words and replace them with the unknown token. We utilize that method to assign the index of \"unk\".\n",
            "\n",
            "Now that we have the vocabulary, we can numerically encode the words in the data using indices from the vocabulary we just created. We also need to pad the sequences so that they're all the same length and we don't run into issues when inputting them into the model."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [],
         "source": [
            "def numericalize_example(obs, vocab):\n",
            "    ids = vocab.lookup_indices(obs[\"tokens\"])\n",
            "    return {\"ids\": ids}\n",
            "\n",
            "\n",
            "train_data = train_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
            "valid_data = valid_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
            "# test_data = test_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [],
         "source": [
            "train_data = train_data.with_format(\"torch\", columns=[\"ids\", \"label\"])\n",
            "valid_data = valid_data.with_format(\"torch\", columns=[\"ids\", \"label\"])\n",
            "# test_data = test_data.with_format(\"torch\", columns = [\"ids\", \"label\"])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "We're going to make use of data loaders. This will allow us to load the data in batches, which will be useful for training the model. This is where we'll use the padding process mentioned above. In this process we need to adequately structure the data so that it can be input into the model. We'll make use of a collate function to do this. Since we're working with PyTorch, we'll create a single tensor for each batch of data and a single tensor for the labels. Note that the data tensor will have a shape of (batch_size, max_length) and the labels tensor will have a shape of (batch_size, 1)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [],
         "source": [
            "pad_index = vocab[\"<pad>\"]\n",
            "\n",
            "\n",
            "def get_collate_fn(pad_index):\n",
            "    def collate_fn(batch):\n",
            "        batch_ids = [doc[\"ids\"] for doc in batch]\n",
            "        batch_ids = nn.utils.rnn.pad_sequence(\n",
            "            batch_ids, padding_value=pad_index, batch_first=True\n",
            "        )\n",
            "        batch_labels = [doc[\"label\"] for doc in batch]\n",
            "        batch_labels = torch.stack(batch_labels)\n",
            "        return {\"ids\": batch_ids, \"label\": batch_labels}\n",
            "\n",
            "    return collate_fn\n",
            "\n",
            "\n",
            "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
            "    collate_fn = get_collate_fn(pad_index)\n",
            "    data_loader = torch.utils.data.DataLoader(\n",
            "        dataset=dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle\n",
            "    )\n",
            "    return data_loader"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we shall set the batch size and create the data loaders for the training, validation and test sets."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "batch_size = 256\n",
            "\n",
            "train_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
            "valid_loader = get_data_loader(valid_data, batch_size, pad_index, shuffle=False)\n",
            "# test_loader = get_data_loader(test_data, batch_size, pad_index, shuffle=False)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "256"
                  ]
               },
               "execution_count": 17,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "len(next(iter(train_loader))[\"ids\"])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Building the model\n",
            "\n",
            "We begin by defining an Embedding layer. Knowing full well that modern NLP models have multiple features, I'll posit that the features required for sentiment analysis are not as complex as those required for other tasks. For that reason, I'll set the embedding dimension to 20. This is a hyperparameter that can be adjusted to improve the model's performance."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 30,
         "metadata": {},
         "outputs": [],
         "source": [
            "class Sentiment(nn.Module):\n",
            "    def __init__(self, vocab_size, embed_dim, pad_index):\n",
            "        super(Sentiment, self).__init__()\n",
            "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_index)\n",
            "        self.fc = nn.Linear(embed_dim, 2)\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.embedding(x)\n",
            "        x = x.mean(dim=1)\n",
            "        x = torch.tanh(x)\n",
            "        x = self.fc(x)\n",
            "\n",
            "        return x"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 31,
         "metadata": {},
         "outputs": [],
         "source": [
            "vocab_size = len(vocab)\n",
            "embed_dim = 20\n",
            "\n",
            "model = Sentiment(vocab_size, embed_dim, pad_index)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 32,
         "metadata": {},
         "outputs": [],
         "source": [
            "# loss function and optimizer\n",
            "criterion = nn.CrossEntropyLoss()\n",
            "optimizer = optim.Adam(model.parameters(), lr=0.001)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 33,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "cpu\n"
               ]
            }
         ],
         "source": [
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "print(device)\n",
            "model = model.to(device)\n",
            "criterion = criterion.to(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 34,
         "metadata": {},
         "outputs": [],
         "source": [
            "# training the model\n",
            "def train(data_loader, model, criterion, optimizer, device):\n",
            "    model.train()\n",
            "    epoch_losses = []\n",
            "    epoch_accuracies = []\n",
            "    for batch in tqdm.tqdm(data_loader, desc=\"Training...\"):\n",
            "        ids = batch[\"ids\"].to(device)\n",
            "        label = batch[\"label\"].to(device)\n",
            "        prediction = model(ids)\n",
            "        loss = criterion(prediction, label)\n",
            "        accuracy = (prediction.argmax(1) == label).sum().item() / len(label)\n",
            "        optimizer.zero_grad()\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "        epoch_losses.append(loss.item())\n",
            "        epoch_accuracies.append(accuracy)\n",
            "    return np.mean(epoch_losses), np.mean(epoch_accuracies)\n",
            "\n",
            "\n",
            "# validation\n",
            "def validate(data_loader, model, criterion, device):\n",
            "    model.eval()\n",
            "    epoch_losses = []\n",
            "    epoch_accuracies = []\n",
            "    with torch.no_grad():\n",
            "        for batch in tqdm.tqdm(data_loader, desc=\"Validating...\"):\n",
            "            ids = batch[\"ids\"].to(device)\n",
            "            label = batch[\"label\"].to(device)\n",
            "            prediction = model(ids)\n",
            "            loss = criterion(prediction, label)\n",
            "            accuracy = (prediction.argmax(1) == label).sum().item() / len(label)\n",
            "            epoch_losses.append(loss.item())\n",
            "            epoch_accuracies.append(accuracy)\n",
            "    return np.mean(epoch_losses), np.mean(epoch_accuracies)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 38,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Training...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1641/1641 [00:24<00:00, 67.40it/s]\n",
                  "Validating...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 547/547 [00:06<00:00, 88.47it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 0\n",
                  "Train Loss: 0.5560, Train Accuracy: 0.7046\n",
                  "Valid Loss: 0.4838, Valid Accuracy: 0.7620\n",
                  "\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Training...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1641/1641 [00:23<00:00, 70.87it/s]\n",
                  "Validating...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 547/547 [00:06<00:00, 89.08it/s] \n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 1\n",
                  "Train Loss: 0.4645, Train Accuracy: 0.7734\n",
                  "Valid Loss: 0.4589, Valid Accuracy: 0.7769\n",
                  "\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Training...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1641/1641 [00:22<00:00, 71.61it/s]\n",
                  "Validating...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 547/547 [00:06<00:00, 87.57it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 2\n",
                  "Train Loss: 0.4480, Train Accuracy: 0.7836\n",
                  "Valid Loss: 0.4520, Valid Accuracy: 0.7808\n",
                  "\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Training...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1641/1641 [00:23<00:00, 70.30it/s]\n",
                  "Validating...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 547/547 [00:06<00:00, 87.23it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 3\n",
                  "Train Loss: 0.4415, Train Accuracy: 0.7879\n",
                  "Valid Loss: 0.4495, Valid Accuracy: 0.7823\n",
                  "\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Training...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1641/1641 [00:23<00:00, 70.37it/s]\n",
                  "Validating...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 547/547 [00:06<00:00, 86.75it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 4\n",
                  "Train Loss: 0.4383, Train Accuracy: 0.7898\n",
                  "Valid Loss: 0.4486, Valid Accuracy: 0.7836\n",
                  "\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Training...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1641/1641 [00:23<00:00, 70.06it/s]\n",
                  "Validating...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 547/547 [00:06<00:00, 88.41it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 5\n",
                  "Train Loss: 0.4364, Train Accuracy: 0.7913\n",
                  "Valid Loss: 0.4482, Valid Accuracy: 0.7838\n",
                  "\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Training...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1641/1641 [00:23<00:00, 70.16it/s]\n",
                  "Validating...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 547/547 [00:06<00:00, 84.30it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 6\n",
                  "Train Loss: 0.4353, Train Accuracy: 0.7919\n",
                  "Valid Loss: 0.4482, Valid Accuracy: 0.7836\n",
                  "\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Training...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1641/1641 [00:24<00:00, 68.36it/s]\n",
                  "Validating...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 547/547 [00:06<00:00, 86.23it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 7\n",
                  "Train Loss: 0.4346, Train Accuracy: 0.7924\n",
                  "Valid Loss: 0.4483, Valid Accuracy: 0.7834\n",
                  "\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Training...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1641/1641 [00:23<00:00, 71.18it/s]\n",
                  "Validating...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 547/547 [00:06<00:00, 89.47it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 8\n",
                  "Train Loss: 0.4339, Train Accuracy: 0.7925\n",
                  "Valid Loss: 0.4483, Valid Accuracy: 0.7835\n",
                  "\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Training...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1641/1641 [00:22<00:00, 72.00it/s]\n",
                  "Validating...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 547/547 [00:06<00:00, 86.11it/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch: 9\n",
                  "Train Loss: 0.4337, Train Accuracy: 0.7926\n",
                  "Valid Loss: 0.4486, Valid Accuracy: 0.7831\n",
                  "\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            }
         ],
         "source": [
            "n_epochs = 10\n",
            "best_valid_loss = float(\"inf\")\n",
            "\n",
            "metrics = collections.defaultdict(list)\n",
            "\n",
            "for epoch in range(n_epochs):\n",
            "    train_loss, train_accuracy = train(\n",
            "        train_loader, model, criterion, optimizer, device\n",
            "    )\n",
            "    valid_loss, valid_accuracy = validate(valid_loader, model, criterion, device)\n",
            "    print(f\"Epoch: {epoch}\")\n",
            "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
            "    print(f\"Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
            "    print()\n",
            "    metrics[\"train_loss\"].append(train_loss)\n",
            "    metrics[\"train_accuracy\"].append(train_accuracy)\n",
            "    metrics[\"valid_loss\"].append(valid_loss)\n",
            "    metrics[\"valid_accuracy\"].append(valid_accuracy)\n",
            "\n",
            "    if valid_loss < best_valid_loss:\n",
            "        best_valid_loss = valid_loss\n",
            "        torch.save(model.state_dict(), \"best_model.pt\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "base",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
