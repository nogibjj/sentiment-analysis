{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert & GPT-2\n",
    "\n",
    "### ECE590 Homework assignment 5\n",
    "Name: Javier Cervantes\n",
    "\n",
    "net id: jc1010\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. We will use the BERT model to perform sentiment analysis. \n",
    "\n",
    "The sentiment is performed via a linear model applied to the output vector above the [CLS] input in BERT (that output vector is referred to as C).\n",
    "\n",
    "Use a pretrained BERT model from:\n",
    "https://github.com/google-research/bert\n",
    "\n",
    "And use sentiment data from:\n",
    "https://huggingface.co/datasets/yelp_polarity\n",
    "\n",
    "Build a sentiment-analysis model based on BERT, using the above data. Do a detailed analysis of performance, and compare the accuracy of this model to results you achieved with the simpler baseline model from the prior homework.\n",
    "\n",
    "Implement the model two ways:\n",
    "\n",
    "(a) Leave all BERT parameters unchanged, and just learn the linear model at the output.\n",
    "\n",
    "(b) Fine-tune all BERT parameters, while also learning the linear layer at the output.\n",
    "\n",
    "In your solution, provide all code and also a detailed summary of the analysis of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start by importing the necessary libraries and downloading the data. Bert has several models available to the public. For this assignment, I'll use the medium Bert uncased model. Additionally, I've been forced to limit the `max sequence length` of each observation to 128 tokens otherwise I'll run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "# existing bert models\n",
    "bert_models = {\n",
    "    \"base\": \"google/bert-base-uncased\",\n",
    "    \"small\": \"google/bert_uncased_L-4_H-512_A-8\",\n",
    "    \"medium\": \"google/bert_uncased_L-8_H-512_A-8\",\n",
    "}\n",
    "\n",
    "# pick a bert model\n",
    "bert_model_name = \"medium\"\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(bert_models[bert_model_name])\n",
    "bert_model = BertModel.from_pretrained(bert_models[bert_model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, I download the `yelp_polarity` dataset, perform train, test and validation splits as well as tokenizing the data in the same way as the `bert` model was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 560000/560000 [03:24<00:00, 2744.46 examples/s]\n",
      "Map: 100%|██████████| 38000/38000 [00:13<00:00, 2738.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "seed = 257\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# load the yelp_polarity dataset\n",
    "train_data, test_data = load_dataset(\"yelp_polarity\", split=[\"train\", \"test\"])\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "\n",
    "# define a function to tokenize the dataset\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"][:max_length], padding=\"max_length\", max_length=max_length\n",
    "    )\n",
    "\n",
    "\n",
    "# tokenize the dataset\n",
    "train_data = train_data.map(tokenize)\n",
    "test_data = test_data.map(tokenize)\n",
    "\n",
    "# convert the dataset to pytorch tensors\n",
    "train_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# validation data\n",
    "train_valid_data = train_data.train_test_split(test_size=0.25)\n",
    "train_data = train_valid_data[\"train\"]\n",
    "valid_data = train_valid_data[\"test\"]\n",
    "\n",
    "# create a dataloader\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=256)\n",
    "test_loader = DataLoader(test_data, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell I've defined a `BertClassifier` class which takes the bert model and adds a linear layer on top of it. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. That will serve as the input to the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# Define a new model with BERT and a linear layer on top for classification\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.classifier = nn.Linear(bert_model.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.pooler_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    for i, batch in tqdm.tqdm(\n",
    "        enumerate(train_loader), desc=\"Training ...\", total=len(train_loader)\n",
    "    ):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy = (outputs.argmax(1) == labels).float().mean()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accuracies.append(accuracy.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accuracies)\n",
    "\n",
    "\n",
    "\n",
    "def validation(model, criterion, valid_loader, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    for i, batch in tqdm.tqdm(\n",
    "        enumerate(valid_loader), desc=\"Validation ...\", total=len(valid_loader)\n",
    "    ):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            accuracy = (outputs.argmax(1) == labels).float().mean()\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accuracies.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accuracies)\n",
    "\n",
    "\n",
    "def test_model(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    for i, batch in tqdm.tqdm(\n",
    "        enumerate(test_loader), desc=\"Testing ...\", total=len(test_loader)\n",
    "    ):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            accuracy = (outputs.argmax(1) == labels).float().mean()\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accuracies.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accuracies)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, criterion, optimizer, train_loader, valid_loader, num_epochs, device\n",
    "):\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train(\n",
    "            model, criterion, optimizer, train_loader, device\n",
    "        )\n",
    "        valid_loss, valid_accuracy = validation(model, criterion, valid_loader, device)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Validation loss: {valid_loss:.4f}\")\n",
    "        print(f\"Validation accuracy: {valid_accuracy:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "        params_grad = [param.requires_grad for param in model.bert_model.parameters()]\n",
    "        if False in params_grad:\n",
    "            full_freeze = \"freeze\"\n",
    "        else:\n",
    "            full_freeze = \"full\"\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(\n",
    "                model.state_dict(), f\"../models/bert_{bert_model_name}_{full_freeze}_{max_length}max.pt\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Leave all BERT parameters unchanged, and just learn the linear model at the output:\n",
    "\n",
    "For this part of the assignment we'll set the `param.requires_grad` to False for all the parameters of the bert model. This way we'll only train the linear layer on top of the bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the model\n",
    "model = BertClassifier(bert_model, num_classes=2)\n",
    "\n",
    "# freeze the BERT parameters so that we only learn the linear classifier\n",
    "for param in model.bert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# define the hyperparameters\n",
    "lr = 2e-5\n",
    "num_epochs = 5\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# move the model and loss function to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [10:36<00:00,  2.58it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [03:21<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Training loss: 0.6224\n",
      "Training accuracy: 0.6697\n",
      "Validation loss: 0.5650\n",
      "Validation accuracy: 0.7306\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [10:37<00:00,  2.57it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [03:21<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Training loss: 0.5518\n",
      "Training accuracy: 0.7322\n",
      "Validation loss: 0.5223\n",
      "Validation accuracy: 0.7497\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [10:38<00:00,  2.57it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [03:21<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Training loss: 0.5242\n",
      "Training accuracy: 0.7454\n",
      "Validation loss: 0.5025\n",
      "Validation accuracy: 0.7582\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [10:38<00:00,  2.57it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [03:21<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Training loss: 0.5099\n",
      "Training accuracy: 0.7523\n",
      "Validation loss: 0.4908\n",
      "Validation accuracy: 0.7643\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [10:38<00:00,  2.57it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [03:21<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Training loss: 0.5009\n",
      "Training accuracy: 0.7570\n",
      "Validation loss: 0.4831\n",
      "Validation accuracy: 0.7682\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_model(model, criterion, optimizer, train_loader, valid_loader, num_epochs=num_epochs, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing ...: 100%|██████████| 149/149 [00:25<00:00,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5489\n",
      "Test accuracy: 0.7172\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "model.load_state_dict(torch.load(f\"../models/bert_medium_freeze_256max.pt\"))\n",
    "\n",
    "test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Fine-tune all BERT parameters, while also learning the linear layer at the output.\n",
    "\n",
    "For this part of the assignment we'll set the `param.requires_grad` to True for all the parameters of the bert model. This way we'll train the linear layer on top of the bert model as well as the bert model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = BertClassifier(bert_model, num_classes=2)\n",
    "\n",
    "# define the hyperparameters\n",
    "lr = 2e-5\n",
    "num_epochs = 3\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# move the model and loss function to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...:   0%|          | 0/1641 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [13:30<00:00,  2.02it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [01:38<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Training loss: 0.3497\n",
      "Training accuracy: 0.8354\n",
      "Validation loss: 0.2978\n",
      "Validation accuracy: 0.8620\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [13:26<00:00,  2.03it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [01:38<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "Training loss: 0.2845\n",
      "Training accuracy: 0.8703\n",
      "Validation loss: 0.2847\n",
      "Validation accuracy: 0.8713\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [13:26<00:00,  2.03it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [01:38<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "Training loss: 0.2541\n",
      "Training accuracy: 0.8859\n",
      "Validation loss: 0.2793\n",
      "Validation accuracy: 0.8757\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "train_model(model, criterion, optimizer, train_loader, valid_loader, num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing ...: 100%|██████████| 149/149 [00:25<00:00,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2790\n",
      "Test accuracy: 0.8782\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test the best model\n",
    "model.load_state_dict(torch.load(f\"../models/bert_medium_full_128max.pt\"))\n",
    "\n",
    "test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluations\n",
    "\n",
    "The model that only learns the linear parameters had underwhelming performance while taking a considerable amount of time to train. This model generated an test accuracy of 72% whereas the model from the previous homework (the model that learned the sentiment embeddings) achieved an accuracy of 89%. \n",
    "\n",
    "The model that fine-tuned the bert parameters as well as the linear layer had a much better performance. This model achieved an accuracy of 88% in 3 epochs (with no signs of overfitting up to that point). This model also took a considerable amount of time and computational resources to train. Given that it barely reached the same performance as the previous homework's model, I would argue that the computational resources are not worth the performance gain.\n",
    "\n",
    "Having said this, there is a very import mention that needs to be raised: we had to limit the max length of the sequence to 128 tokens due to computational limitations. Even in the previous homework, limiting the max sequence length had considerable a negative impact in the model's performance. Another compromise we had to make was that we used the `medium` bert instead of `base` bert. This model was optimized on far fewer parameters. I'm sure both of these compromises had a negative impact on the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. This task investigates database retrieval via BERT.\n",
    "\n",
    "Consider a database of your choosing, consisting of a large database of documents. Examples are (but use what you want):\n",
    "\n",
    "https://www.kaggle.com/datasets/rowhitswami/nips-papers-1987-2019-updated\n",
    "https://commoncrawl.org/\n",
    "https://www.kaggle.com/datasets/crawford/20-newsgroups\n",
    "\n",
    "Using BERT, encode each of the documents in your corpus to a vector. Do this with “out of the box” BERT, with no changes to the base model (taken from the above GitHub)\n",
    "\n",
    "Build code that, given a question or prompt, will pull appropriate documents from your database. This also should be done with BERT applied to the prompt, and using that write code that pulls from the database the top-10 best matches, ranked by probability of match.\n",
    "\n",
    "In your solution, provide all code and also a detailed summary of the analysis of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've chosen to work with the 20 Newsroups Datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "documents = newsgroups.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [02:58<00:00, 105.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the GPU if one is available\n",
    "bert_model = BertModel.from_pretrained(bert_models[bert_model_name])\n",
    "\n",
    "bert_model.to(device)\n",
    "\n",
    "# This will hold all the document vectors\n",
    "document_vectors = []\n",
    "\n",
    "# Wrap your loop with tqdm for a progress bar\n",
    "for document in tqdm(documents):\n",
    "    # Tokenize the document, return_tensors='pt' tells the tokenizer to return PyTorch tensors\n",
    "    inputs = tokenizer(document, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Move the inputs to the GPU if one is available\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "\n",
    "    # Get the BERT embeddings for the document\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # Use the embeddings of the [CLS] token as the document vector\n",
    "    document_vector = outputs.last_hidden_state[:, 0, :]\n",
    "    document_vectors.append(document_vector.to('cpu').numpy()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../models/document_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(document_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
