{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert & GPT-2\n",
    "\n",
    "### ECE590 Homework assignment 5\n",
    "Name: Javier Cervantes\n",
    "\n",
    "net id: jc1010\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. We will use the BERT model to perform sentiment analysis. \n",
    "\n",
    "The sentiment is performed via a linear model applied to the output vector above the [CLS] input in BERT (that output vector is referred to as C).\n",
    "\n",
    "Use a pretrained BERT model from:\n",
    "https://github.com/google-research/bert\n",
    "\n",
    "And use sentiment data from:\n",
    "https://huggingface.co/datasets/yelp_polarity\n",
    "\n",
    "Build a sentiment-analysis model based on BERT, using the above data. Do a detailed analysis of performance, and compare the accuracy of this model to results you achieved with the simpler baseline model from the prior homework.\n",
    "\n",
    "Implement the model two ways:\n",
    "\n",
    "(a) Leave all BERT parameters unchanged, and just learn the linear model at the output.\n",
    "\n",
    "(b) Fine-tune all BERT parameters, while also learning the linear layer at the output.\n",
    "\n",
    "In your solution, provide all code and also a detailed summary of the analysis of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start by importing the necessary libraries and downloading the data. Bert has several models available to the public. For this assignment, I'll use the medium Bert uncased model. Additionally, I've been forced to limit the `max sequence length` of each observation to 128 tokens otherwise I'll run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "# existing bert models\n",
    "bert_models = {\n",
    "    \"base\": \"google/bert-base-uncased\",\n",
    "    \"small\": \"google/bert_uncased_L-4_H-512_A-8\",\n",
    "    \"medium\": \"google/bert_uncased_L-8_H-512_A-8\",\n",
    "}\n",
    "\n",
    "# pick a bert model\n",
    "bert_model_name = \"medium\"\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(bert_models[bert_model_name])\n",
    "bert_model = BertModel.from_pretrained(bert_models[bert_model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, I download the `yelp_polarity` dataset, perform train, test and validation splits as well as tokenizing the data in the same way as the `bert` model was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 560000/560000 [03:24<00:00, 2744.46 examples/s]\n",
      "Map: 100%|██████████| 38000/38000 [00:13<00:00, 2738.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "seed = 257\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# load the yelp_polarity dataset\n",
    "train_data, test_data = load_dataset(\"yelp_polarity\", split=[\"train\", \"test\"])\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "\n",
    "# define a function to tokenize the dataset\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"][:max_length], padding=\"max_length\", max_length=max_length\n",
    "    )\n",
    "\n",
    "\n",
    "# tokenize the dataset\n",
    "train_data = train_data.map(tokenize)\n",
    "test_data = test_data.map(tokenize)\n",
    "\n",
    "# convert the dataset to pytorch tensors\n",
    "train_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# validation data\n",
    "train_valid_data = train_data.train_test_split(test_size=0.25)\n",
    "train_data = train_valid_data[\"train\"]\n",
    "valid_data = train_valid_data[\"test\"]\n",
    "\n",
    "# create a dataloader\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=256)\n",
    "test_loader = DataLoader(test_data, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell I've defined a `BertClassifier` class which takes the bert model and adds a linear layer on top of it. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. That will serve as the input to the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# Define a new model with BERT and a linear layer on top for classification\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.classifier = nn.Linear(bert_model.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.pooler_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    for i, batch in tqdm.tqdm(\n",
    "        enumerate(train_loader), desc=\"Training ...\", total=len(train_loader)\n",
    "    ):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy = (outputs.argmax(1) == labels).float().mean()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accuracies.append(accuracy.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accuracies)\n",
    "\n",
    "\n",
    "def validation(model, criterion, valid_loader, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    for i, batch in tqdm.tqdm(\n",
    "        enumerate(valid_loader), desc=\"Validation ...\", total=len(valid_loader)\n",
    "    ):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            accuracy = (outputs.argmax(1) == labels).float().mean()\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accuracies.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accuracies)\n",
    "\n",
    "\n",
    "def test_model(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    for i, batch in tqdm.tqdm(\n",
    "        enumerate(test_loader), desc=\"Testing ...\", total=len(test_loader)\n",
    "    ):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            accuracy = (outputs.argmax(1) == labels).float().mean()\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accuracies.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accuracies)\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, criterion, optimizer, train_loader, valid_loader, num_epochs, device\n",
    "):\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train(\n",
    "            model, criterion, optimizer, train_loader, device\n",
    "        )\n",
    "        valid_loss, valid_accuracy = validation(model, criterion, valid_loader, device)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Validation loss: {valid_loss:.4f}\")\n",
    "        print(f\"Validation accuracy: {valid_accuracy:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "        params_grad = [param.requires_grad for param in model.bert_model.parameters()]\n",
    "        if False in params_grad:\n",
    "            full_freeze = \"freeze\"\n",
    "        else:\n",
    "            full_freeze = \"full\"\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                f\"../models/bert_{bert_model_name}_{full_freeze}_{max_length}max.pt\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Leave all BERT parameters unchanged, and just learn the linear model at the output:\n",
    "\n",
    "For this part of the assignment we'll set the `param.requires_grad` to False for all the parameters of the bert model. This way we'll only train the linear layer on top of the bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = BertClassifier(bert_model, num_classes=2)\n",
    "\n",
    "# freeze the BERT parameters so that we only learn the linear classifier\n",
    "for param in model.bert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# define the hyperparameters\n",
    "lr = 2e-5\n",
    "num_epochs = 5\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# move the model and loss function to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [10:36<00:00,  2.58it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [03:21<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Training loss: 0.6224\n",
      "Training accuracy: 0.6697\n",
      "Validation loss: 0.5650\n",
      "Validation accuracy: 0.7306\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [10:37<00:00,  2.57it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [03:21<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Training loss: 0.5518\n",
      "Training accuracy: 0.7322\n",
      "Validation loss: 0.5223\n",
      "Validation accuracy: 0.7497\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [10:38<00:00,  2.57it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [03:21<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Training loss: 0.5242\n",
      "Training accuracy: 0.7454\n",
      "Validation loss: 0.5025\n",
      "Validation accuracy: 0.7582\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [10:38<00:00,  2.57it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [03:21<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Training loss: 0.5099\n",
      "Training accuracy: 0.7523\n",
      "Validation loss: 0.4908\n",
      "Validation accuracy: 0.7643\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [10:38<00:00,  2.57it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [03:21<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Training loss: 0.5009\n",
      "Training accuracy: 0.7570\n",
      "Validation loss: 0.4831\n",
      "Validation accuracy: 0.7682\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing ...: 100%|██████████| 149/149 [00:25<00:00,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5489\n",
      "Test accuracy: 0.7172\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "model.load_state_dict(torch.load(f\"../models/bert_medium_freeze_256max.pt\"))\n",
    "\n",
    "test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Fine-tune all BERT parameters, while also learning the linear layer at the output.\n",
    "\n",
    "For this part of the assignment we'll set the `param.requires_grad` to True for all the parameters of the bert model. This way we'll train the linear layer on top of the bert model as well as the bert model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = BertClassifier(bert_model, num_classes=2)\n",
    "\n",
    "# define the hyperparameters\n",
    "lr = 2e-5\n",
    "num_epochs = 3\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# move the model and loss function to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...:   0%|          | 0/1641 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [13:30<00:00,  2.02it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [01:38<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Training loss: 0.3497\n",
      "Training accuracy: 0.8354\n",
      "Validation loss: 0.2978\n",
      "Validation accuracy: 0.8620\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [13:26<00:00,  2.03it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [01:38<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "Training loss: 0.2845\n",
      "Training accuracy: 0.8703\n",
      "Validation loss: 0.2847\n",
      "Validation accuracy: 0.8713\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ...: 100%|██████████| 1641/1641 [13:26<00:00,  2.03it/s]\n",
      "Validation ...: 100%|██████████| 547/547 [01:38<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "Training loss: 0.2541\n",
      "Training accuracy: 0.8859\n",
      "Validation loss: 0.2793\n",
      "Validation accuracy: 0.8757\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing ...: 100%|██████████| 149/149 [00:25<00:00,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2790\n",
      "Test accuracy: 0.8782\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test the best model\n",
    "model.load_state_dict(torch.load(f\"../models/bert_medium_full_128max.pt\"))\n",
    "\n",
    "test_loss, test_accuracy = test_model(model, criterion, test_loader, device)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluations\n",
    "\n",
    "The model that only learns the linear parameters had underwhelming performance while taking a considerable amount of time to train. This model generated an test accuracy of 72% whereas the model from the previous homework (the model that learned the sentiment embeddings) achieved an accuracy of 89%. \n",
    "\n",
    "The model that fine-tuned the bert parameters as well as the linear layer had a much better performance. This model achieved an accuracy of 88% in 3 epochs (with no signs of overfitting up to that point). This model also took a considerable amount of time and computational resources to train. Given that it barely reached the same performance as the previous homework's model, I would argue that the computational resources are not worth the performance gain.\n",
    "\n",
    "Having said this, there is a very import mention that needs to be raised: we had to limit the max length of the sequence to 128 tokens due to computational limitations. Even in the previous homework, limiting the max sequence length had considerable a negative impact in the model's performance. Another compromise we had to make was that we used the `medium` bert instead of `base` bert. This model was optimized on far fewer parameters. I'm sure both of these compromises had a negative impact on the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. This task investigates database retrieval via BERT.\n",
    "\n",
    "Consider a database of your choosing, consisting of a large database of documents. Examples are (but use what you want):\n",
    "\n",
    "https://www.kaggle.com/datasets/rowhitswami/nips-papers-1987-2019-updated\n",
    "https://commoncrawl.org/\n",
    "https://www.kaggle.com/datasets/crawford/20-newsgroups\n",
    "\n",
    "Using BERT, encode each of the documents in your corpus to a vector. Do this with “out of the box” BERT, with no changes to the base model (taken from the above GitHub)\n",
    "\n",
    "Build code that, given a question or prompt, will pull appropriate documents from your database. This also should be done with BERT applied to the prompt, and using that write code that pulls from the database the top-10 best matches, ranked by probability of match.\n",
    "\n",
    "In your solution, provide all code and also a detailed summary of the analysis of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset=\"all\")\n",
    "documents = newsgroups.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [02:58<00:00, 105.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the GPU if one is available\n",
    "bert_model = BertModel.from_pretrained(bert_models[bert_model_name])\n",
    "\n",
    "bert_model.to(device)\n",
    "\n",
    "# This will hold all the document vectors\n",
    "document_vectors = []\n",
    "\n",
    "# Wrap your loop with tqdm for a progress bar\n",
    "for document in tqdm(documents):\n",
    "    # Tokenize the document, return_tensors='pt' tells the tokenizer to return PyTorch tensors\n",
    "    inputs = tokenizer(\n",
    "        document, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "    )\n",
    "\n",
    "    # Move the inputs to the GPU if one is available\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "\n",
    "    # Get the BERT embeddings for the document\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # Use the embeddings of the [CLS] token as the document vector\n",
    "    document_vector = outputs.last_hidden_state[:, 0, :]\n",
    "    document_vectors.append(document_vector.to(\"cpu\").numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('../models/document_vectors.pkl', 'wb') as f:\n",
    "#     pickle.dump(document_vectors, f)\n",
    "\n",
    "with open(\"../models/document_vectors.pkl\", \"rb\") as f:\n",
    "    document_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# given a prompt, find the 10 most similar documents\n",
    "def find_similar_documents(prompt, document_vectors, documents, tokenizer, bert_model):\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n",
    "    )\n",
    "\n",
    "    # Get the BERT embeddings for the prompt\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # Use the embeddings of the [CLS] token as the prompt vector\n",
    "    prompt_vector = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    # Calculate the cosine similarity between the prompt vector and all document vectors\n",
    "    similarities = np.dot(document_vectors, prompt_vector.T)\n",
    "\n",
    "    # Sort the documents by their similarity to the prompt\n",
    "    most_similar = np.argsort(np.squeeze(similarities), axis=0)[::-1]\n",
    "\n",
    "    # Convert documents to an array\n",
    "    documents = np.array(documents)\n",
    "\n",
    "    # Return the 10 most similar documents\n",
    "    return [documents[i] for i in most_similar[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(bert_models[bert_model_name])\n",
    "bert_model = BertModel.from_pretrained(bert_models[bert_model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've chosen to work with the 20 Newsgroups Datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: cs902060@ariel.yorku.ca (GEOFFREY E DIAS)\n",
      "Subject: How does a pitcher get a save?\n",
      "Organization: York University, Toronto, Canada\n",
      "Lines: 4\n",
      "\n",
      "\n",
      "\tThe subject line says it all. What is the rule that qualifies\n",
      "a pitcher as making a save?\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "From: enolan@sharkbite.esd.sgi.com (Ed Nolan)\n",
      "Subject: Devils and Islanders tiebreaker????\n",
      "Organization: Silicon Graphics, Inc.\n",
      "Lines: 4\n",
      "Nntp-Posting-Host: sharkbite.esd.sgi.com\n",
      "\n",
      "If the Islanders beat the Devils tonight, they would finish with\n",
      "identical records.  Who's the lucky team that gets to face the Penguins\n",
      "in the opening round?   Also, can somebody list the rules for breaking\n",
      "ties.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "From: cal2d@csissun11.ee.Virginia.EDU (Craig Allen Lorie)\n",
      "Subject: Re: Devils and Islanders tiebreaker????\n",
      "Organization: University of Virginia\n",
      "Lines: 15\n",
      "\n",
      "According to the hockey gurus over at ESPN, should the Islanders win tonite\n",
      "the two teams will have the same record, but the Devils will be playing the\n",
      "Penguins.  This is because the Islanders have won the season series against\n",
      "the Devils.  I think the rules for deciding a tie breaker include:\n",
      "\n",
      "1.  season series\n",
      "2.  goals against\n",
      "3.  goals for\n",
      "\n",
      "in this order (correct me if I'm wrong).  Anyone have anything to add?\n",
      "\n",
      "Craig\n",
      "\n",
      "Go Islanders!\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "From: jclouse@discover.wright.edu (Jim Clouse)\n",
      "Subject: World Series Stats\n",
      "Nntp-Posting-Host: discgate\n",
      "Organization: Wright State University\n",
      "X-Newsreader: TIN [version 1.1 PL8]\n",
      "Lines: 3\n",
      "\n",
      "Does anybody else think that WS stats should become part of\n",
      "a player's career stats?   Why not?\n",
      " \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Subject: Best Sportwriters...\n",
      "From: csc2imd@cabell.vcu.edu (Ian M. Derby)\n",
      "Expires: Sat, 1 May 1993 04:00:00 GMT\n",
      "Organization: Virginia Commonwealth University\n",
      "Keywords: Sportswriters\n",
      "Summary: Sportswriters\n",
      "Lines: 19\n",
      "\n",
      "\n",
      "Since someone brought up sports radio, howabout sportswriting???\n",
      "\n",
      "(Anyone give an opinion) \n",
      "\n",
      "Which city do you think has the best sports coverage in terms of\n",
      "print media? \n",
      "\n",
      "(these are general questions) \n",
      "\n",
      "Is the Washington Post better than the Philadelphia Inquier or the NY\n",
      "Times?  \n",
      "\n",
      "Howabout the Philadelphia Daily News compared to the New York Daily\n",
      "News?  \n",
      "\n",
      "\n",
      "Do you notice papers being subjective or objective to the home team?\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "From: gt4722a@prism.gatech.EDU (James B. Atkins)\n",
      "Subject: Honda Mailing list?\n",
      "Organization: Georgia Institute of Technology\n",
      "Lines: 2\n",
      "\n",
      "\n",
      "\tIs there a Honda mailing list, and if so how do I subscribe to it?\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "From: z_millerwl@ccsvax.sfasu.edu\n",
      "Subject: ASTROS FOR REAL?\n",
      "Organization: Stephen F. Austin State University\n",
      "Lines: 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WHO THINKS THE ASTROS ARE GOING PLACES???\n",
      "THEY'RE CURRENTLY FIRST PLACE.\n",
      "THEY'RE 5-4, 5-1 ON THE ROAD! \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Organization: Central Michigan University\n",
      "From: Martin D. Hill <32GFKKH@CMUVM.CSV.CMICH.EDU>\n",
      "Subject:    college hockey all-star game\n",
      "Lines: 3\n",
      "\n",
      "Does anybody know the details of the Shriners All-Star game that featured the\n",
      "best seniors in college hockey in a game in Orono, Maine?  If you do, please\n",
      "reply.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "From: barryf@hpfcso.FC.HP.COM (Barry Fowler)\n",
      "Subject: Re: Impala SS going into production!\n",
      "Organization: HP Colorado Computer Mfg. Operation\n",
      "Lines: 1\n",
      "\n",
      "Does that mean that they're gonna bring back the Biscayne and Bel Air?\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "From: marty@howdy.wustl.edu (Marty Olevitch)\n",
      "Subject: Re: Jewish Baseball Players?\n",
      "Nntp-Posting-Host: howdy\n",
      "Organization: Washington U. Physics Dept\n",
      "Lines: 3\n",
      "\n",
      "Bo Bilinsky?\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# given a prompt, find the 10 most similar documents\n",
    "prompt = \"Who do you pick: Islanders, Penguins or Bruins?\"\n",
    "similar_documents = find_similar_documents(\n",
    "    prompt, document_vectors, documents, tokenizer, bert_model\n",
    ")\n",
    "for i in similar_documents:\n",
    "    print(i)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the previous results, we can see that our document retrieval function is on the right track: it correctly identifies that I'm talking about sports and not about religion, politics, computers or any of the other subjects included in my dataset.\n",
    ">\n",
    "> Having said that, I'm a bit sceptical about the results. It returned documents from baseball as well as hockey. Given that these are presumably the 10 most likely documents to match my prompt, I can say that I'm somewhat underwhelmed by the results. I would have expected the documents to be more closely related to the prompt. \n",
    ">\n",
    "> As compared to the task in question 1: here we still used the Bert Medium model but didn't limit the max sequence length (at least not below what the model can take). There is definitely some loss of accuracy from using the medium model as compared to using the base model but we're still dealing with a pretty large model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use the GPT-2 code from:\n",
    "\n",
    "https://huggingface.co/openai-community/gpt2\n",
    "or\n",
    "\n",
    "https://github.com/openai/gpt-2\n",
    "\n",
    "Using results from Problem 2, put as input to GPT2 the prompt alone, and evaluate the quality of the answer. Separately, input the prompt plus the document pulled for the database. Examine and evaluate the (somewhat subjective) quality of the generated text, with and without the context provided by the pulled document.\n",
    "\n",
    "Since the context length of GPT2 is limited, you may have to consider smaller-size documents in your context database. This can be done by considering smaller documents (e.g., abstracts from the NeurIPS) database, or breaking the larger documents into smaller pieces (and encoding each to a vector via BERT).\n",
    "\n",
    "In your solution, provide all code and also a detailed summary of the analysis of the results. This part will require a descent level of software/coding expertise. Teaming is fine. For some this part of the assignment may be a “reach;” do your best, and focus on learning from this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First we shall input the prompt alone and evaluate the quality of the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "def ask_gpt2(prompt, max_length=50, num_return_sequences=5):\n",
    "    # initialize tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    # encode context the generation is conditioned on\n",
    "    encoding = tokenizer.encode_plus(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "\n",
    "    # create attention mask\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        # max_length=max_length + input_ids.shape[-1],\n",
    "        max_new_tokens=max_length + input_ids.shape[-1],\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # decode the output and use .shape[-1] so it doesn't return the prompt\n",
    "    return [\n",
    "        tokenizer.decode(output[i][input_ids.shape[-1] :], skip_special_tokens=True)\n",
    "        for i in range(num_return_sequences)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert line breaks to make the output more readable\n",
    "def insert_newlines(string, every=64):\n",
    "    return \"\\n\".join(string[i : i + every] for i in range(0, len(string), every))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I know who the Islanders are and I'd pick them. I just like th\n",
      "e way they play. They've been really good to me. You can watch a\n",
      " game and you would never know just how good they are. So, I kno\n",
      "w that I can get a good look at them and then I will pick you up\n",
      " and pick me up.\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "answer = ask_gpt2(prompt, max_length=1024, num_return_sequences=1)[0]\n",
    "print(insert_newlines(answer, every=64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The answer seems quite reasonable. Not only is it answering my question, it's providing a reason for its answer. Sometimes the answer looks like decent English but doesn't really make sense in the context of what I'm asking but overall, it's decent.\n",
    ">\n",
    "> Now we shall input the prompt plus the documents pulled from the database and evaluate the quality of the generated text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the prompt and the pulled documents\n",
    "new_prompt = prompt + \" \" + \" \".join(similar_documents[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". From the point of view of a fan who is about to watch the game, I would be\n",
      "truly surprised if the tiebreakers could be broken.\n",
      "\n",
      "-From: rwj@hockey-reference.org (RwJ)\n",
      "\n",
      "\n",
      "\n",
      "A few thoughts on the teams:\n",
      "\n",
      "--The Penguins and Devils are the two remaining teams in the series,\n",
      "(there are only 16 games left, so the playoff spot is\n",
      "*not* theirs.)\n",
      "\n",
      "\"Bold\" has been a popular term for them since 2000, and\n",
      "\"brave\" seems to be the most popular word in general. It has gone from being used as\n",
      "-a\n",
      "'very' common phrase in hockey to being a'very important word\n",
      "and' frequently used by NHL managers to mean 'it's very important'\n",
      ". The team is often used to describe the players who are supposed to take the lead, or to\n",
      "...\n",
      "\n",
      "\n",
      "I don't think it's as simple as \"in order to win, every team has to play one game before\n",
      "being eliminated by the other team\".\n",
      "\n",
      "(Also, there is no point in telling anyone that the New York Islanders\n",
      "would win the second round of the playoffs, as the Isles are\n",
      " still opponent #2 in this series.) \"Brave\", on top of that, is generally used\n",
      "to describe teams that have been eliminated from the Stanley Cup Playoffs.\n"
     ]
    }
   ],
   "source": [
    "answer = ask_gpt2(new_prompt, max_length=1024, num_return_sequences=1)[0]\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The answers when adding the top 3 documents (I couldn't use more than 3 because I was running into some IndexErrors that I couldn't solve) to the original prompt now look exactly like the messages in the documents. They seem like email answers. You can even see that the output sometimes contains email addresses. Even the format of the text is similar to that contained in the documents (lines are much shorter and the use of `\\n` is quite evident). \n",
    ">\n",
    "> As such, the output doesn't contain an answer to my question. That might be understandable given that my question was a small piece of the new prompt which contained 3 entire documents.\n",
    ">\n",
    "> Having said that, I still see some hallucinations in the output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
